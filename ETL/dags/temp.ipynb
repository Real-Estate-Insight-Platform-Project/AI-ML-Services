{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceca7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "from get_bq_data import get_bq_data\n",
    "from model_trainer_1 import get_predictions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS' ] = 'service_keys.json'\n",
    "\n",
    "client = bigquery. Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e827056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data():\n",
    "    \"\"\"Load dataset, aggregate, and push to Supabase\"\"\" \n",
    "\n",
    "    existing_data = get_bq_data(client,\"state_market\") \n",
    "\n",
    "    return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb47b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(df):\n",
    "#     \"\"\"Load dataset, train model, save predictions to Supabase\"\"\"\n",
    "\n",
    "#     # List of features to predict\n",
    "#     features = [\n",
    "#         \"median_listing_price\",\n",
    "#         \"average_listing_price\",\n",
    "#         \"median_listing_price_per_square_foot\",\n",
    "#         \"total_listing_count\",\n",
    "#         \"median_days_on_market\"\n",
    "#     ]\n",
    "    \n",
    "#     for feature in features:\n",
    "#         get_predictions(df, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dcff5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>state_num</th>\n",
       "      <th>median_listing_price</th>\n",
       "      <th>median_listing_price_mm</th>\n",
       "      <th>median_listing_price_yy</th>\n",
       "      <th>active_listing_count</th>\n",
       "      <th>active_listing_count_mm</th>\n",
       "      <th>active_listing_count_yy</th>\n",
       "      <th>median_days_on_market</th>\n",
       "      <th>...</th>\n",
       "      <th>median_square_feet_yy</th>\n",
       "      <th>average_listing_price</th>\n",
       "      <th>average_listing_price_mm</th>\n",
       "      <th>average_listing_price_yy</th>\n",
       "      <th>total_listing_count</th>\n",
       "      <th>total_listing_count_mm</th>\n",
       "      <th>total_listing_count_yy</th>\n",
       "      <th>pending_ratio</th>\n",
       "      <th>pending_ratio_mm</th>\n",
       "      <th>pending_ratio_yy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>198060</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>26519</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>-0.1296</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>264342</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>28563</td>\n",
       "      <td>-0.004288</td>\n",
       "      <td>-0.1169</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.067376</td>\n",
       "      <td>0.0179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>279375</td>\n",
       "      <td>-0.006667</td>\n",
       "      <td>-0.0333</td>\n",
       "      <td>3710</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>-0.0196</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0278</td>\n",
       "      <td>327328</td>\n",
       "      <td>-0.003374</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>3710</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>-0.0196</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>312000</td>\n",
       "      <td>-0.018868</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>26807</td>\n",
       "      <td>-0.026015</td>\n",
       "      <td>-0.1148</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>477781</td>\n",
       "      <td>-0.029825</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>34886</td>\n",
       "      <td>-0.034965</td>\n",
       "      <td>-0.0948</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>-0.048711</td>\n",
       "      <td>0.0294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>169900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>15717</td>\n",
       "      <td>-0.013309</td>\n",
       "      <td>-0.1057</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>235007</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>18446</td>\n",
       "      <td>-0.013161</td>\n",
       "      <td>-0.0939</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>549000</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>71686</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>-0.1431</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>1030673</td>\n",
       "      <td>-0.016759</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>106530</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>-0.1262</td>\n",
       "      <td>0.4944</td>\n",
       "      <td>-0.069627</td>\n",
       "      <td>0.0314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5044</th>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>21543</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.2931</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>639805</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>31820</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.4810</td>\n",
       "      <td>-0.011600</td>\n",
       "      <td>-0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>648900</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>24463</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>0.2562</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0035</td>\n",
       "      <td>887686</td>\n",
       "      <td>-0.012300</td>\n",
       "      <td>-0.0109</td>\n",
       "      <td>33936</td>\n",
       "      <td>-0.010900</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.3888</td>\n",
       "      <td>-0.011200</td>\n",
       "      <td>-0.1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>261500</td>\n",
       "      <td>-0.027900</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>4245</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>325854</td>\n",
       "      <td>-0.029700</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>6712</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>-0.010600</td>\n",
       "      <td>-0.0729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>392325</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>13190</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0036</td>\n",
       "      <td>506718</td>\n",
       "      <td>-0.004000</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>18863</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>-0.049800</td>\n",
       "      <td>-0.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048</th>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>472500</td>\n",
       "      <td>-0.040600</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>2833</td>\n",
       "      <td>-0.019700</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>1194779</td>\n",
       "      <td>-0.031400</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>3414</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>-0.0233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5049 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  state_num  median_listing_price  median_listing_price_mm  \\\n",
       "0     2017      7          1                198060                -0.004223   \n",
       "1     2017      7          2                279375                -0.006667   \n",
       "2     2017      7          3                312000                -0.018868   \n",
       "3     2017      7          4                169900                 0.000000   \n",
       "4     2017      7          5                549000                -0.001637   \n",
       "...    ...    ...        ...                   ...                      ...   \n",
       "5044  2025      9         47                450000                 0.000000   \n",
       "5045  2025      9         48                648900                -0.001600   \n",
       "5046  2025      9         49                261500                -0.027900   \n",
       "5047  2025      9         50                392325                -0.016700   \n",
       "5048  2025      9         51                472500                -0.040600   \n",
       "\n",
       "      median_listing_price_yy  active_listing_count  active_listing_count_mm  \\\n",
       "0                      0.0654                 26519                 0.002343   \n",
       "1                     -0.0333                  3710                 0.092784   \n",
       "2                      0.0576                 26807                -0.026015   \n",
       "3                      0.0534                 15717                -0.013309   \n",
       "4                      0.0660                 71686                 0.037319   \n",
       "...                       ...                   ...                      ...   \n",
       "5044                   0.0228                 21543                 0.028400   \n",
       "5045                  -0.0016                 24463                -0.002500   \n",
       "5046                   0.0898                  4245                 0.038700   \n",
       "5047                   0.0257                 13190                 0.037400   \n",
       "5048                  -0.0151                  2833                -0.019700   \n",
       "\n",
       "      active_listing_count_yy  median_days_on_market  ...  \\\n",
       "0                     -0.1296                     75  ...   \n",
       "1                     -0.0196                     53  ...   \n",
       "2                     -0.1148                     56  ...   \n",
       "3                     -0.1057                     73  ...   \n",
       "4                     -0.1431                     40  ...   \n",
       "...                       ...                    ...  ...   \n",
       "5044                   0.2931                     45  ...   \n",
       "5045                   0.2562                     57  ...   \n",
       "5046                   0.1546                     52  ...   \n",
       "5047                   0.0899                     44  ...   \n",
       "5048                   0.1312                     69  ...   \n",
       "\n",
       "      median_square_feet_yy  average_listing_price  average_listing_price_mm  \\\n",
       "0                    0.0233                 264342                 -0.000707   \n",
       "1                   -0.0278                 327328                 -0.003374   \n",
       "2                    0.0086                 477781                 -0.029825   \n",
       "3                    0.0245                 235007                  0.003810   \n",
       "4                   -0.0005                1030673                 -0.016759   \n",
       "...                     ...                    ...                       ...   \n",
       "5044                 0.0005                 639805                  0.012100   \n",
       "5045                -0.0035                 887686                 -0.012300   \n",
       "5046                 0.0120                 325854                 -0.029700   \n",
       "5047                -0.0036                 506718                 -0.004000   \n",
       "5048                -0.0101                1194779                 -0.031400   \n",
       "\n",
       "      average_listing_price_yy  total_listing_count  total_listing_count_mm  \\\n",
       "0                       0.0543                28563               -0.004288   \n",
       "1                      -0.0121                 3710                0.092784   \n",
       "2                       0.0699                34886               -0.034965   \n",
       "3                       0.0652                18446               -0.013161   \n",
       "4                       0.1016               106530                0.009907   \n",
       "...                        ...                  ...                     ...   \n",
       "5044                    0.0062                31820                0.022700   \n",
       "5045                   -0.0109                33936               -0.010900   \n",
       "5046                    0.0233                 6712                0.030100   \n",
       "5047                    0.0422                18863                0.000800   \n",
       "5048                    0.0381                 3414               -0.001800   \n",
       "\n",
       "      total_listing_count_yy  pending_ratio  pending_ratio_mm  \\\n",
       "0                    -0.1169         0.0789         -0.067376   \n",
       "1                    -0.0196         0.0000          0.000000   \n",
       "2                    -0.0948         0.2988         -0.048711   \n",
       "3                    -0.0939         0.1771          0.002831   \n",
       "4                    -0.1262         0.4944         -0.069627   \n",
       "...                      ...            ...               ...   \n",
       "5044                  0.2011         0.4810         -0.011600   \n",
       "5045                  0.1543         0.3888         -0.011200   \n",
       "5046                  0.1029         0.5822         -0.010600   \n",
       "5047                  0.0766         0.4327         -0.049800   \n",
       "5048                  0.1068         0.2077          0.013300   \n",
       "\n",
       "      pending_ratio_yy  \n",
       "0               0.0179  \n",
       "1               0.0000  \n",
       "2               0.0294  \n",
       "3               0.0145  \n",
       "4               0.0314  \n",
       "...                ...  \n",
       "5044           -0.1128  \n",
       "5045           -0.1228  \n",
       "5046           -0.0729  \n",
       "5047           -0.0216  \n",
       "5048           -0.0233  \n",
       "\n",
       "[5049 rows x 45 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = aggregate_data()\n",
    "df.sort_values(by=['year', 'month', 'state_num'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7dbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_2 import preprocess_data_2\n",
    "\n",
    "target_df = preprocess_data_2(25, df.copy())\n",
    "prediction_df = target_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ecf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, prediction_df):\n",
    "    \"\"\"Load dataset, train model, save predictions to Supabase\"\"\"\n",
    "\n",
    "    features = [\n",
    "        \"median_listing_price\",\n",
    "        \"median_days_on_market\"\n",
    "    ]\n",
    "    \n",
    "    for feature in features:\n",
    "        predictions = get_predictions(df, feature, 24)\n",
    "        prediction_df[feature] = predictions\n",
    "\n",
    "    for col in features:\n",
    "        if col in prediction_df.columns:\n",
    "            prediction_df[col] = prediction_df[col].astype(int)\n",
    "\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddbbe098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory: 1.80 MB\n",
      "Final memory: 1.20 MB\n",
      "Reduced by 33.2%\n",
      "Validation RMSE: 25758.3877, RMSLE: 0.0576, MAE: 20027.8076, MAPE: 4.53%, R²: 0.9684\n",
      "Validation RMSE: 24591.6228, RMSLE: 0.0579, MAE: 19875.8159, MAPE: 4.63%, R²: 0.9707\n",
      "Validation RMSE: 24912.9814, RMSLE: 0.0585, MAE: 20113.1023, MAPE: 4.70%, R²: 0.9691\n",
      "Validation RMSE: 23030.3157, RMSLE: 0.0582, MAE: 19681.4635, MAPE: 4.81%, R²: 0.9735\n",
      "Validation RMSE: 21073.4184, RMSLE: 0.0485, MAE: 16258.8128, MAPE: 3.77%, R²: 0.9783\n",
      "Validation RMSE: 37272.8521, RMSLE: 0.0910, MAE: 31277.6786, MAPE: 7.34%, R²: 0.9331\n",
      "Validation RMSE: 48528.4525, RMSLE: 0.1063, MAE: 39186.1468, MAPE: 8.77%, R²: 0.8870\n",
      "Validation RMSE: 60313.0797, RMSLE: 0.1443, MAE: 53695.0836, MAPE: 12.08%, R²: 0.8332\n",
      "Validation RMSE: 77465.1050, RMSLE: 0.1804, MAE: 69075.0726, MAPE: 15.23%, R²: 0.7142\n",
      "Validation RMSE: 91817.1577, RMSLE: 0.2163, MAE: 81119.1544, MAPE: 17.78%, R²: 0.5991\n",
      "Validation RMSE: 59468.9872, RMSLE: 0.1317, MAE: 51184.2574, MAPE: 11.10%, R²: 0.8245\n",
      "Validation RMSE: 64754.6291, RMSLE: 0.1393, MAE: 54609.3480, MAPE: 11.71%, R²: 0.7927\n",
      "Validation RMSE: 87566.4345, RMSLE: 0.2029, MAE: 76891.7693, MAPE: 17.04%, R²: 0.6176\n",
      "Validation RMSE: 52216.6100, RMSLE: 0.1197, MAE: 44851.1060, MAPE: 10.25%, R²: 0.8595\n",
      "Validation RMSE: 56253.0437, RMSLE: 0.1369, MAE: 44981.7889, MAPE: 10.47%, R²: 0.8279\n",
      "Validation RMSE: 49708.1017, RMSLE: 0.1170, MAE: 39353.9347, MAPE: 9.17%, R²: 0.8648\n",
      "Validation RMSE: 50653.2442, RMSLE: 0.1124, MAE: 41833.1884, MAPE: 9.22%, R²: 0.8648\n",
      "Validation RMSE: 53861.0929, RMSLE: 0.1245, MAE: 45223.1875, MAPE: 10.29%, R²: 0.8543\n",
      "Validation RMSE: 89353.5477, RMSLE: 0.2302, MAE: 80325.5080, MAPE: 18.50%, R²: 0.6003\n",
      "Validation RMSE: 87893.5708, RMSLE: 0.2188, MAE: 79525.2065, MAPE: 17.95%, R²: 0.6206\n",
      "Validation RMSE: 79899.9807, RMSLE: 0.1981, MAE: 72056.7785, MAPE: 16.28%, R²: 0.6788\n",
      "Validation RMSE: 75047.4221, RMSLE: 0.1854, MAE: 63515.9687, MAPE: 14.45%, R²: 0.7089\n",
      "Validation RMSE: 74452.1828, RMSLE: 0.1841, MAE: 64184.9133, MAPE: 14.74%, R²: 0.6979\n",
      "Validation RMSE: 70015.6830, RMSLE: 0.1706, MAE: 60362.7387, MAPE: 13.87%, R²: 0.7336\n",
      "🏃 View run XGB_Darts_Model_median_listing_price at: http://127.0.0.1:5000/#/experiments/1/runs/590ff629966248d8947613c78f888142\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.380794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.389910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.399229\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.409683\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.422354\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.437655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.456196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.477270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.497987\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.516342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.533878\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.549332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.562691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.574619\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.585905\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.597963\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.611756\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.628131\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.646538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.666218\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.685533\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.702961\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.717503\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 162754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.730016\n",
      "Validation RMSE: 12832.7438, RMSLE: 0.0288, MAE: 9167.2924, MAPE: 2.13%, R²: 0.9922\n",
      "Validation RMSE: 27372.0374, RMSLE: 0.0667, MAE: 21422.7442, MAPE: 5.11%, R²: 0.9637\n",
      "Validation RMSE: 27919.2680, RMSLE: 0.0692, MAE: 22581.7007, MAPE: 5.46%, R²: 0.9611\n",
      "Validation RMSE: 33631.5733, RMSLE: 0.0838, MAE: 27900.1229, MAPE: 6.77%, R²: 0.9435\n",
      "Validation RMSE: 34065.0555, RMSLE: 0.0863, MAE: 28616.1109, MAPE: 6.95%, R²: 0.9433\n",
      "Validation RMSE: 43539.3443, RMSLE: 0.1056, MAE: 36131.7101, MAPE: 8.46%, R²: 0.9086\n",
      "Validation RMSE: 39753.6119, RMSLE: 0.0960, MAE: 33120.2046, MAPE: 7.71%, R²: 0.9241\n",
      "Validation RMSE: 52315.8949, RMSLE: 0.1196, MAE: 46129.4516, MAPE: 10.25%, R²: 0.8745\n",
      "Validation RMSE: 77152.9680, RMSLE: 0.1845, MAE: 70193.7393, MAPE: 15.74%, R²: 0.7165\n",
      "Validation RMSE: 81334.0436, RMSLE: 0.1967, MAE: 73929.9320, MAPE: 16.60%, R²: 0.6854\n",
      "Validation RMSE: 75608.3170, RMSLE: 0.1776, MAE: 68706.7768, MAPE: 15.33%, R²: 0.7164\n",
      "Validation RMSE: 88524.8962, RMSLE: 0.2065, MAE: 79447.3705, MAPE: 17.63%, R²: 0.6126\n",
      "Validation RMSE: 82153.2325, RMSLE: 0.1955, MAE: 73969.3213, MAPE: 16.67%, R²: 0.6635\n",
      "Validation RMSE: 53158.4403, RMSLE: 0.1272, MAE: 46480.6674, MAPE: 10.85%, R²: 0.8544\n",
      "Validation RMSE: 54994.2854, RMSLE: 0.1305, MAE: 46067.7100, MAPE: 10.67%, R²: 0.8355\n",
      "Validation RMSE: 52505.4616, RMSLE: 0.1229, MAE: 42943.4156, MAPE: 9.83%, R²: 0.8491\n",
      "Validation RMSE: 55381.0310, RMSLE: 0.1264, MAE: 45747.1286, MAPE: 10.25%, R²: 0.8384\n",
      "Validation RMSE: 60754.2783, RMSLE: 0.1386, MAE: 50612.8380, MAPE: 11.36%, R²: 0.8147\n",
      "Validation RMSE: 78170.3968, RMSLE: 0.1939, MAE: 68461.2347, MAPE: 15.66%, R²: 0.6941\n",
      "Validation RMSE: 87849.8666, RMSLE: 0.2147, MAE: 79192.8283, MAPE: 17.73%, R²: 0.6209\n",
      "Validation RMSE: 83262.4470, RMSLE: 0.2027, MAE: 73829.2970, MAPE: 16.56%, R²: 0.6512\n",
      "Validation RMSE: 75494.9254, RMSLE: 0.1869, MAE: 65008.0959, MAPE: 14.88%, R²: 0.7054\n",
      "Validation RMSE: 70816.2873, RMSLE: 0.1744, MAE: 61642.1594, MAPE: 14.18%, R²: 0.7267\n",
      "Validation RMSE: 67508.0353, RMSLE: 0.1656, MAE: 58973.5938, MAPE: 13.65%, R²: 0.7524\n",
      "🏃 View run LightGBM_Darts_Model_median_listing_price at: http://127.0.0.1:5000/#/experiments/1/runs/e7dd3fc4088b4b8789414dbe66b092ac\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Using LightGBM model for predictions\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.561769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.572509\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.582925\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.593560\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.604684\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.616716\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.629950\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.644003\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.657168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.669063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.680377\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.690578\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.699494\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.707474\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.714796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.722223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.730307\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.739603\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.749945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.760864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.771413\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.780834\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.788638\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164529\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.795352\n",
      "Initial memory: 1.80 MB\n",
      "Final memory: 1.20 MB\n",
      "Reduced by 33.2%\n",
      "Validation RMSE: 3.1589, RMSLE: 0.0638, MAE: 2.4273, MAPE: 5.02%, R²: 0.8791\n",
      "Validation RMSE: 6.3269, RMSLE: 0.1114, MAE: 5.1235, MAPE: 9.79%, R²: 0.5448\n",
      "Validation RMSE: 9.4267, RMSLE: 0.1498, MAE: 7.2555, MAPE: 10.87%, R²: 0.0808\n",
      "Validation RMSE: 11.8374, RMSLE: 0.1700, MAE: 9.2549, MAPE: 12.35%, R²: -0.2248\n",
      "Validation RMSE: 14.1609, RMSLE: 0.2399, MAE: 11.3577, MAPE: 17.01%, R²: 0.0982\n",
      "Validation RMSE: 6.1034, RMSLE: 0.1318, MAE: 4.4707, MAPE: 9.24%, R²: 0.6616\n",
      "Validation RMSE: 6.5124, RMSLE: 0.1423, MAE: 5.3595, MAPE: 12.43%, R²: 0.5135\n",
      "Validation RMSE: 7.0536, RMSLE: 0.1703, MAE: 5.3619, MAPE: 12.35%, R²: 0.4578\n",
      "Validation RMSE: 7.7414, RMSLE: 0.1761, MAE: 5.6497, MAPE: 12.57%, R²: 0.2789\n",
      "Validation RMSE: 8.3113, RMSLE: 0.1929, MAE: 6.4129, MAPE: 13.13%, R²: 0.1328\n",
      "Validation RMSE: 9.9619, RMSLE: 0.2047, MAE: 7.7887, MAPE: 14.55%, R²: -0.0839\n",
      "Validation RMSE: 14.1024, RMSLE: 0.2887, MAE: 11.0492, MAPE: 19.27%, R²: -0.7707\n",
      "Validation RMSE: 14.0598, RMSLE: 0.2663, MAE: 11.3278, MAPE: 19.13%, R²: -0.4694\n",
      "Validation RMSE: 13.0749, RMSLE: 0.2245, MAE: 10.6856, MAPE: 16.53%, R²: -0.3519\n",
      "Validation RMSE: 11.2288, RMSLE: 0.1639, MAE: 9.1325, MAPE: 12.61%, R²: -0.0421\n",
      "Validation RMSE: 13.5302, RMSLE: 0.1886, MAE: 11.6010, MAPE: 14.82%, R²: -0.3188\n",
      "Validation RMSE: 19.3216, RMSLE: 0.2928, MAE: 15.5323, MAPE: 21.21%, R²: -0.4183\n",
      "Validation RMSE: 13.7791, RMSLE: 0.2814, MAE: 11.5186, MAPE: 20.87%, R²: -0.0579\n",
      "Validation RMSE: 14.6868, RMSLE: 0.3569, MAE: 11.9568, MAPE: 23.83%, R²: -0.8422\n",
      "Validation RMSE: 15.9612, RMSLE: 0.3991, MAE: 12.9856, MAPE: 26.47%, R²: -1.4246\n",
      "Validation RMSE: 14.0125, RMSLE: 0.2939, MAE: 10.9472, MAPE: 20.28%, R²: -0.6781\n",
      "Validation RMSE: 15.8774, RMSLE: 0.3158, MAE: 12.6997, MAPE: 21.92%, R²: -1.1102\n",
      "Validation RMSE: 16.7062, RMSLE: 0.3097, MAE: 13.5813, MAPE: 22.15%, R²: -1.2170\n",
      "Validation RMSE: 19.2877, RMSLE: 0.3638, MAE: 15.7992, MAPE: 24.75%, R²: -1.5006\n",
      "🏃 View run XGB_Darts_Model_median_days_on_market at: http://127.0.0.1:5000/#/experiments/1/runs/1727444f979244989c5c5aa32adffc2b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.538262\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.533512\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.531969\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.531928\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.521530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.501035\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.472060\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.439296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.411856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.396629\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.385763\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.375931\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.369908\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.370559\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.379196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.391989\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.400610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.399537\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.390019\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.375371\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.365814\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.365779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.372056\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 166805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428, number of used features: 662\n",
      "[LightGBM] [Info] Start training from score 0.381950\n",
      "Validation RMSE: 2.5545, RMSLE: 0.0495, MAE: 2.0494, MAPE: 4.19%, R²: 0.9210\n",
      "Validation RMSE: 7.0692, RMSLE: 0.1231, MAE: 5.4086, MAPE: 10.23%, R²: 0.4317\n",
      "Validation RMSE: 7.5154, RMSLE: 0.1217, MAE: 5.6401, MAPE: 8.69%, R²: 0.4158\n",
      "Validation RMSE: 9.9412, RMSLE: 0.1456, MAE: 7.6518, MAPE: 10.34%, R²: 0.1362\n",
      "Validation RMSE: 12.4851, RMSLE: 0.2081, MAE: 10.2012, MAPE: 15.40%, R²: 0.2990\n",
      "Validation RMSE: 6.7812, RMSLE: 0.1482, MAE: 5.3141, MAPE: 10.71%, R²: 0.5822\n",
      "Validation RMSE: 6.6824, RMSLE: 0.1396, MAE: 5.6135, MAPE: 12.51%, R²: 0.4878\n",
      "Validation RMSE: 5.7236, RMSLE: 0.1369, MAE: 4.1603, MAPE: 9.82%, R²: 0.6430\n",
      "Validation RMSE: 6.8202, RMSLE: 0.1621, MAE: 4.6090, MAPE: 10.34%, R²: 0.4403\n",
      "Validation RMSE: 8.7457, RMSLE: 0.2018, MAE: 6.9600, MAPE: 14.28%, R²: 0.0398\n",
      "Validation RMSE: 10.4123, RMSLE: 0.2210, MAE: 8.0337, MAPE: 15.09%, R²: -0.1842\n",
      "Validation RMSE: 13.6482, RMSLE: 0.2720, MAE: 10.7987, MAPE: 18.86%, R²: -0.6585\n",
      "Validation RMSE: 13.8396, RMSLE: 0.2601, MAE: 11.0623, MAPE: 18.62%, R²: -0.4238\n",
      "Validation RMSE: 10.8068, RMSLE: 0.1802, MAE: 8.9289, MAPE: 13.94%, R²: 0.0765\n",
      "Validation RMSE: 10.6494, RMSLE: 0.1530, MAE: 8.8257, MAPE: 12.29%, R²: 0.0627\n",
      "Validation RMSE: 12.3667, RMSLE: 0.1718, MAE: 10.5683, MAPE: 13.69%, R²: -0.1017\n",
      "Validation RMSE: 16.7055, RMSLE: 0.2539, MAE: 14.1286, MAPE: 19.91%, R²: -0.0602\n",
      "Validation RMSE: 15.9260, RMSLE: 0.3372, MAE: 13.0266, MAPE: 23.65%, R²: -0.4132\n",
      "Validation RMSE: 15.7298, RMSLE: 0.3847, MAE: 13.0377, MAPE: 26.04%, R²: -1.1131\n",
      "Validation RMSE: 15.1365, RMSLE: 0.3586, MAE: 12.2373, MAPE: 24.45%, R²: -1.1806\n",
      "Validation RMSE: 14.6667, RMSLE: 0.3119, MAE: 11.4984, MAPE: 21.27%, R²: -0.8385\n",
      "Validation RMSE: 17.6390, RMSLE: 0.3645, MAE: 14.3182, MAPE: 24.88%, R²: -1.6044\n",
      "Validation RMSE: 20.4173, RMSLE: 0.4039, MAE: 17.1090, MAPE: 28.13%, R²: -2.3114\n",
      "Validation RMSE: 19.8177, RMSLE: 0.3724, MAE: 16.4242, MAPE: 25.84%, R²: -1.6399\n",
      "🏃 View run LightGBM_Darts_Model_median_days_on_market at: http://127.0.0.1:5000/#/experiments/1/runs/98ac646b865943b586d7de96538cc365\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Using XGBoost model for predictions\n"
     ]
    }
   ],
   "source": [
    "preds = train_model(df, prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4653e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = preds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5acac6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_df = df.groupby(\"state_num\").tail(1).reset_index(drop=True)\n",
    "preds['year_month'] = (preds['year'] - 2025) * 12 + preds['month'] - 9\n",
    "preds.drop(columns=['year', 'month'], inplace=True)\n",
    "cols_order = ['year_month', 'state_num', 'median_listing_price', 'median_days_on_market']\n",
    "preds = preds[cols_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d084fe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>state_num</th>\n",
       "      <th>median_listing_price</th>\n",
       "      <th>median_days_on_market</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>309099.0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>410331.0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>483349.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>278332.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>721191.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>504757.2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>705543.6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>286576.8</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>400951.2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>502608.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year_month  state_num  median_listing_price  median_days_on_market\n",
       "102           3          1              309099.0                     79\n",
       "103           3          2              410331.0                     99\n",
       "104           3          3              483349.0                     70\n",
       "105           3          4              278332.0                     77\n",
       "106           3          5              721191.0                     69\n",
       "..          ...        ...                   ...                    ...\n",
       "607          12         47              504757.2                     48\n",
       "608          12         48              705543.6                     47\n",
       "609          12         49              286576.8                     62\n",
       "610          12         50              400951.2                     48\n",
       "611          12         51              502608.0                     64\n",
       "\n",
       "[153 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds[preds['year_month'].isin([3,6,12])]\n",
    "preds.loc[preds['year_month'] == 12, 'median_listing_price'] = preds.loc[preds['year_month'] == 12, 'median_listing_price'] * 1.15\n",
    "preds.loc[preds['year_month'] == 6, 'median_listing_price'] = preds.loc[preds['year_month'] == 6, 'median_listing_price'] * 1.1\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf20f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['appreciation'] = 0.0\n",
    "for index, row in preds.iterrows():\n",
    "    state_num = row['state_num']\n",
    "    \n",
    "    current_price = latest_df[(latest_df['state_num'] == state_num)]['median_listing_price'].values[0]\n",
    "    predicted_price = row['median_listing_price']\n",
    "    \n",
    "    appreciation = ((predicted_price - current_price) / current_price) * 100\n",
    "    preds.loc[index, 'appreciation'] = appreciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0885332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_volatility(df):\n",
    "    volatility = {}\n",
    "    for state in df['state_num'].unique():\n",
    "        state_data = df[df['state_num'] == state].sort_values(by=['year', 'month'])\n",
    "        if len(state_data) >= 12:\n",
    "            last_12_mm = state_data['median_listing_price_mm'].tail(12).values\n",
    "            volatility[state] = np.std(last_12_mm) * 100\n",
    "        else:\n",
    "            volatility[state] = np.nan  # Not enough data to calculate volatility\n",
    "    return volatility\n",
    "\n",
    "volatility_data = calculate_volatility(df)\n",
    "\n",
    "preds['volatility'] = 0.0  # Initialize Volatility column\n",
    "\n",
    "for index, row in preds.iterrows():\n",
    "    state_num = row['state_num']\n",
    "    if state_num in volatility_data:\n",
    "        preds.loc[index, 'volatility'] = volatility_data[state_num]\n",
    "    else:\n",
    "        preds.loc[index, 'volatility'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d8d34",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "incompatible index of inserted column with frame index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:12694\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12693\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 12694\u001b[0m     reindexed_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m  12695\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m  12696\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:5164\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5147\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5148\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m   5149\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5162\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 5164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:5629\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5628\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5630\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   5631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:5652\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5651\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5652\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[0;32m   5654\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5656\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4440\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4438\u001b[0m             indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m-> 4440\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_reindex_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2736\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2736\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2738\u001b[0m     \u001b[38;5;66;03m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:223\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_or_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:618\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    616\u001b[0m         tuples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tuples\u001b[38;5;241m.\u001b[39m_values)\n\u001b[1;32m--> 618\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuples_to_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tuples, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mpandas/_libs/lib.pyx:3056\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26804\\4203774329.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'liquidity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'liquidity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_days_on_market\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26804\\4203774329.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(preds)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnormalize_days_on_market\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year_month'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'median_days_on_market'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'liquidity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'liquidity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'liquidity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4312\u001b[0m             \u001b[1;31m# Column to set is duplicated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4313\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4314\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4315\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4316\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4526\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mTimeSeries\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconformed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrames\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4527\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4528\u001b[0m         \"\"\"\n\u001b[1;32m-> 4529\u001b[1;33m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4531\u001b[0m         if (\n\u001b[0;32m   4532\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5266\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5268\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5269\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5270\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12698\u001b[0m             \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12699\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12701\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m  12702\u001b[0m             \u001b[1;34m\"incompatible index of inserted column with frame index\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12703\u001b[0m         ) from err\n\u001b[0;32m  12704\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreindexed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def normalize_days_on_market(preds):\n",
    "    # Use transform so the inverse values align with the original index\n",
    "    inv_days = preds.groupby('year_month')['median_days_on_market'].transform(\n",
    "        lambda s: np.where(s != 0, 1 / s, 0)\n",
    "    )\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    preds = preds.copy()\n",
    "    preds['liquidity'] = scaler.fit_transform(inv_days.to_frame()) * 100\n",
    "    return preds\n",
    "\n",
    "preds = normalize_days_on_market(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf55af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
