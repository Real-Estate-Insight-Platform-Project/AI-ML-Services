{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceca7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from get_bq_data import get_bq_data\n",
    "from preprocessing_3 import preprocess_data_3\n",
    "from upload_bq_data import upload_bq_data\n",
    "from preprocessing_4 import preprocess_data_4\n",
    "from model_trainer_2 import get_predictions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS' ] = 'service_keys.json'\n",
    "\n",
    "client = bigquery. Client()\n",
    "\n",
    "DATA_PATH = \"/tmp/data3.csv\" \n",
    "DATA_PATH2 = \"/tmp/data4.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb47b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    \"\"\"Download CSV dataset from URL\"\"\"\n",
    "    url = \"https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/RDC_Inventory_Core_Metrics_County.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def aggregate_data(df):\n",
    "    \"\"\"Load dataset, aggregate, and push to Supabase\"\"\" \n",
    "\n",
    "    county_lookup = get_bq_data(client,\"county_lookup\") \n",
    "    state_lookup = get_bq_data(client,\"state_lookup\") \n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_data_3(df, county_lookup, state_lookup)\n",
    "\n",
    "    existing_data = get_bq_data(client,\"county_market\") \n",
    "\n",
    "    # Create year_month composite keys for checking\n",
    "    existing_data['year_month'] = existing_data['year'].astype(str) + '_' + existing_data['month'].astype(str)\n",
    "    df['year_month'] = df['year'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "    # Filter out records that already exist in the database\n",
    "    existing_year_months = set(existing_data['year_month'])\n",
    "    \n",
    "    if df['year_month'].isin(existing_year_months).all():\n",
    "        print(\"All records already exist in the database. No new data to insert.\")\n",
    "        existing_data = existing_data.drop('year_month', axis=1)\n",
    "        new_data = existing_data.copy()\n",
    "        # raise AirflowSkipException(\"Skipping remaining tasks as no new data was found\")\n",
    "    else:\n",
    "        df = df.drop('year_month', axis=1)\n",
    "        existing_data = existing_data.drop('year_month', axis=1)\n",
    "        new_data = pd.concat([existing_data, df]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        upload_bq_data(client, \"county_market\", df, \"WRITE_APPEND\")\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def train_model(new_data):\n",
    "    \"\"\"Load dataset, train model, save predictions to Supabase\"\"\"\n",
    "    df = new_data\n",
    "\n",
    "    # List of features to predict\n",
    "    features = [\n",
    "        \"median_listing_price\",\n",
    "        \"average_listing_price\",\n",
    "        \"median_listing_price_per_square_foot\",\n",
    "        \"total_listing_count\",\n",
    "        \"median_days_on_market\"\n",
    "    ]\n",
    "\n",
    "    target_df = preprocess_data_4(df.copy())\n",
    "    prediction_df = target_df.copy()\n",
    "\n",
    "    for feature in features:\n",
    "        predictions = get_predictions(df, feature)\n",
    "        prediction_df[feature] = predictions\n",
    "\n",
    "    # Add market_trend column based on average_listing_price trend\n",
    "    prediction_df['market_trend'] = 'stable'\n",
    "\n",
    "    for county_num in prediction_df['county_num'].unique():\n",
    "        county_data = prediction_df[prediction_df['county_num'] == county_num]\n",
    "\n",
    "        # Check if we have at least 3 months of predictions to analyze trend\n",
    "        if len(county_data) >= 3:\n",
    "            # Get the average_listing_price for the next 3 months\n",
    "            prices = county_data['average_listing_price'].iloc[:3].values\n",
    "\n",
    "            # Calculate if trend is rising, declining or stable\n",
    "            if prices[2] > prices[0]:\n",
    "                prediction_df.loc[prediction_df['county_num'] == county_num, 'market_trend'] = 'rising'\n",
    "            elif prices[2] < prices[0]:\n",
    "                prediction_df.loc[prediction_df['county_num'] == county_num, 'market_trend'] = 'declining'\n",
    "\n",
    "    for col in features:\n",
    "        if col in prediction_df.columns:\n",
    "            prediction_df[col] = prediction_df[col].astype(int)\n",
    "\n",
    "    upload_bq_data(client, \"county_predictions\", prediction_df, \"WRITE_TRUNCATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db48a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c19d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records already exist in the database. No new data to insert.\n"
     ]
    }
   ],
   "source": [
    "df = aggregate_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec71a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to predict\n",
    "# features = [\n",
    "#         \"median_listing_price\",\n",
    "#         \"average_listing_price\",\n",
    "#         \"median_listing_price_per_square_foot\",\n",
    "#         \"total_listing_count\",\n",
    "#         \"median_days_on_market\"\n",
    "# ]\n",
    "features = [\n",
    "        \"median_listing_price\"\n",
    "]\n",
    "\n",
    "target_df = preprocess_data_4(df.copy())\n",
    "prediction_df = target_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf562081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory: 108.08 MB\n",
      "Final memory: 73.09 MB\n",
      "Reduced by 32.4%\n",
      "Validation RMSE: 25356.7090, RMSLE: 0.0858, MAE: 14061.1813, MAPE: 4.80%, R²: 0.9874\n",
      "Validation RMSE: 32044.1651, RMSLE: 0.1080, MAE: 18982.1832, MAPE: 6.53%, R²: 0.9795\n",
      "Validation RMSE: 38246.0142, RMSLE: 0.1233, MAE: 21436.9404, MAPE: 7.58%, R²: 0.9717\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.122634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 254001\n",
      "[LightGBM] [Info] Number of data points in the train set: 209230, number of used features: 998\n",
      "[LightGBM] [Info] Start training from score 0.573377\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.192056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 254001\n",
      "[LightGBM] [Info] Number of data points in the train set: 209230, number of used features: 998\n",
      "[LightGBM] [Info] Start training from score 0.580722\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.234326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 254001\n",
      "[LightGBM] [Info] Number of data points in the train set: 209230, number of used features: 998\n",
      "[LightGBM] [Info] Start training from score 0.588224\n",
      "Validation RMSE: 25377.0068, RMSLE: 0.0833, MAE: 13809.4590, MAPE: 4.65%, R²: 0.9874\n",
      "Validation RMSE: 31796.6522, RMSLE: 0.1060, MAE: 18667.9695, MAPE: 6.39%, R²: 0.9798\n",
      "Validation RMSE: 38922.8495, RMSLE: 0.1234, MAE: 21274.3946, MAPE: 7.58%, R²: 0.9707\n",
      "Using XGBoost model for predictions\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    predictions = get_predictions(df, feature)\n",
    "    prediction_df[feature] = predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
