{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95e60087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer, Scaler, InvertibleMapper\n",
    "from darts.models import LightGBMModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5250e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_root():\n",
    "    return Path().resolve().parent.parent\n",
    "\n",
    "# set mlflow tracking uri\n",
    "mlflow.set_tracking_uri(uri=(get_project_root() / 'AI-ML-Services' / 'forecasting_engine' / 'mlruns').as_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b737987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_id = mlflow.create_experiment(\"RealEstate_Price_Prediction\", tags={\n",
    "#     \"topic\":\"experiment-management\",\n",
    "#     \"version\": \"v1\"\n",
    "# })\n",
    "\n",
    "# print(f\"Experiment created with ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13720c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/1%20Disk%20D/Mora%20Academic/Sem%205/Data%20Science%20and%20Engineering%20Project/AI-ML-Services/forecasting_engine/mlruns/571155472720974951', creation_time=1757264864212, experiment_id='571155472720974951', last_update_time=1757264864212, lifecycle_stage='active', name='RealEstate_Price_Prediction', tags={'topic': 'experiment-management', 'version': 'v1'}>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We set the experiment to the one we created earlier.\n",
    "mlflow.set_experiment(experiment_name=\"RealEstate_Price_Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d412e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9a919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"data/State_Processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c139b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df.copy()\n",
    "df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2) + '-01')\n",
    "# sort\n",
    "df = df.sort_values(['state','date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976afdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd9332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ts = {}\n",
    "grouped_static = {}   # optional static covariates per state\n",
    "\n",
    "for state, g in df.groupby('state'):\n",
    "    # create a TimeSeries with monthly frequency\n",
    "    ts = TimeSeries.from_dataframe(\n",
    "    g,\n",
    "    time_col=\"date\",\n",
    "    value_cols=\"median_listing_price\",\n",
    "    freq=\"MS\",\n",
    "    )\n",
    "    grouped_ts[state] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a119c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_cov_ts = {}\n",
    "future_cov_ts = {}\n",
    "past_cov_cols = ['median_listing_price_mm',\n",
    "    'median_listing_price_yy', 'active_listing_count',\n",
    "    'active_listing_count_mm', 'active_listing_count_yy',\n",
    "    'median_days_on_market', 'median_days_on_market_mm',\n",
    "    'median_days_on_market_yy', 'new_listing_count', 'new_listing_count_mm',\n",
    "    'new_listing_count_yy', 'price_increased_count',\n",
    "    'price_increased_count_mm', 'price_increased_count_yy',\n",
    "    'price_increased_share', 'price_increased_share_mm',\n",
    "    'price_increased_share_yy', 'price_reduced_count',\n",
    "    'price_reduced_count_mm', 'price_reduced_count_yy',\n",
    "    'price_reduced_share', 'price_reduced_share_mm',\n",
    "    'price_reduced_share_yy', 'pending_listing_count',\n",
    "    'pending_listing_count_mm', 'pending_listing_count_yy',\n",
    "    'median_listing_price_per_square_foot',\n",
    "    'median_listing_price_per_square_foot_mm',\n",
    "    'median_listing_price_per_square_foot_yy', 'median_square_feet',\n",
    "    'median_square_feet_mm', 'median_square_feet_yy',\n",
    "    'average_listing_price', 'average_listing_price_mm',\n",
    "    'average_listing_price_yy', 'total_listing_count',\n",
    "    'total_listing_count_mm', 'total_listing_count_yy', 'pending_ratio',\n",
    "    'pending_ratio_mm', 'pending_ratio_yy']\n",
    "future_cov_cols = ['month','year']  # calendar features known ahead\n",
    "\n",
    "for state, g in df.groupby('state'):\n",
    "    # Past covariates as a multivariate TimeSeries\n",
    "    if all(c in g.columns for c in past_cov_cols):\n",
    "        past_cov_ts[state] = TimeSeries.from_dataframe(g, time_col='date', value_cols=past_cov_cols, freq='MS')\n",
    "    else:\n",
    "        past_cov_ts[state] = None\n",
    "\n",
    "    if all(c in g.columns for c in future_cov_cols):\n",
    "        future_cov_ts[state] = TimeSeries.from_dataframe(g, time_col='date', value_cols=future_cov_cols, freq='MS')\n",
    "    else:\n",
    "        future_cov_ts[state] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ce6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {}\n",
    "ts_transformed = {}\n",
    "\n",
    "for state in grouped_ts:\n",
    "    log_transformer = InvertibleMapper(np.log1p, np.expm1)   # log1p for target, invertible\n",
    "    scaler = Scaler()\n",
    "    pipe = Pipeline([log_transformer, scaler])\n",
    "    # fit_transform expects a TimeSeries (or list); we pass the one series\n",
    "    transformed = pipe.fit_transform(grouped_ts[state])\n",
    "    pipeline_dict[state] = pipe\n",
    "    ts_transformed[state] = transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f041d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list = [ts_transformed[s] for s in ts_transformed]\n",
    "past_cov_list = [past_cov_ts[s] for s in ts_transformed]\n",
    "future_cov_list = [future_cov_ts[s] for s in ts_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394aa171",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_predict = 3\n",
    "train_series = []\n",
    "val_series = []\n",
    "train_pasts = []\n",
    "val_pasts = []\n",
    "train_futures = []\n",
    "val_futures = []\n",
    "test_futures = []\n",
    "\n",
    "for s in ts_transformed:\n",
    "    ts = ts_transformed[s]\n",
    "    # if len(ts) < 24:\n",
    "    #     # skip too-short series (optional) or handle differently\n",
    "    #     continue\n",
    "    train = ts[:-n_predict]\n",
    "    val = ts[-n_predict:]\n",
    "    train_series.append(train)\n",
    "    val_series.append(val)\n",
    "    # same slicing for covariates if present\n",
    "    if past_cov_ts[s] is not None:\n",
    "        train_pasts.append(past_cov_ts[s][:-n_predict])\n",
    "        val_pasts.append(past_cov_ts[s][-n_predict:])\n",
    "    else:\n",
    "        train_pasts.append(None)\n",
    "        val_pasts.append(None)\n",
    "    if future_cov_ts[s] is not None:\n",
    "        train_futures.append(future_cov_ts[s][:-n_predict])\n",
    "        val_futures.append(future_cov_ts[s][-n_predict:])\n",
    "        test_futures.append(future_cov_ts[s])  # for final test prediction\n",
    "    else:\n",
    "        train_futures.append(None)\n",
    "        val_futures.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0791131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250261\n",
      "[LightGBM] [Info] Number of data points in the train set: 3621, number of used features: 998\n",
      "[LightGBM] [Info] Start training from score 0.642702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 4945.9197, RMSLE: 0.0092, MAE: 3632.5093, MAPE: 0.75%, R²: 0.9987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/17 00:12:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run(run_name=\"LightGBM_Darts_Model\"):\n",
    "\n",
    "    # Log model hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"lags\": 12,\n",
    "        \"lags_past_covariates\": list(range(-24, 0)),\n",
    "        \"lags_future_covariates\": list(range(1, 2)),\n",
    "        \"output_chunk_length\": n_predict,\n",
    "        \"random_state\": 42\n",
    "    })\n",
    "\n",
    "    lgbm_model = LightGBMModel(\n",
    "        lags=12,\n",
    "        lags_past_covariates=list(range(-24, 0)),\n",
    "        lags_future_covariates=list(range(1, 2)),\n",
    "        output_chunk_length=n_predict,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    lgbm_model.fit(\n",
    "        series=train_series,\n",
    "        past_covariates=train_pasts,\n",
    "        future_covariates=train_futures\n",
    "    )\n",
    "\n",
    "    preds = lgbm_model.predict(\n",
    "        n=n_predict,\n",
    "        series=train_series,\n",
    "        past_covariates=train_pasts,\n",
    "        future_covariates=test_futures\n",
    "    )\n",
    "\n",
    "    y_true, y_hat = [], []\n",
    "    for i, sname in enumerate(ts_transformed):\n",
    "        pred_ts = preds[i]\n",
    "        inv = pipeline_dict[sname].inverse_transform(pred_ts)\n",
    "        y_hat.append(inv.values()[-1].item())\n",
    "\n",
    "        true_val = val_series[i]\n",
    "        true_inv = pipeline_dict[sname].inverse_transform(true_val)\n",
    "        y_true.append(true_inv.values()[-1].item())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_hat = np.array(y_hat)\n",
    "\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "    rmsle = math.sqrt(((np.log1p(np.maximum(0, y_hat)) - np.log1p(np.maximum(0, y_true)))**2).mean())\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    mape = np.mean(np.abs((y_true - y_hat) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "    print(f\"Validation RMSE: {rmse:.4f}, RMSLE: {rmsle:.4f}, \"\n",
    "          f\"MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"RMSE\": rmse,\n",
    "        \"RMSLE\": rmsle,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "    # Log trained model\n",
    "    mlflow.lightgbm.log_model(lgbm_model.model, artifact_path=\"darts_lgbm_model\")\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250298\n",
      "[LightGBM] [Info] Number of data points in the train set: 3417, number of used features: 1002\n",
      "[LightGBM] [Info] Start training from score 0.628580\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250298\n",
      "[LightGBM] [Info] Number of data points in the train set: 3417, number of used features: 1002\n",
      "[LightGBM] [Info] Start training from score 0.636690\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250298\n",
      "[LightGBM] [Info] Number of data points in the train set: 3417, number of used features: 1002\n",
      "[LightGBM] [Info] Start training from score 0.645559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMModel(lags=12, lags_past_covariates=[-24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1], lags_future_covariates=[1, 2, 3], output_chunk_length=3, output_chunk_shift=0, add_encoders=None, likelihood=None, quantiles=None, random_state=42, multi_models=True, use_static_covariates=True, categorical_past_covariates=None, categorical_future_covariates=None, categorical_static_covariates=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lags = 12\n",
    "lags_past_covariates = list(range(-24,0))   # previous 24 months of past covariates\n",
    "lags_future_covariates = list(range(1, 2))  # months ahead \n",
    "\n",
    "lgbm_model = LightGBMModel(\n",
    "    lags=lags,\n",
    "    lags_past_covariates=lags_past_covariates,\n",
    "    lags_future_covariates=lags_future_covariates,\n",
    "    output_chunk_length=n_predict,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on the training series list (global model trained across all states)\n",
    "lgbm_model.fit(series=train_series, past_covariates=train_pasts, future_covariates=train_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53492ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def calculate_metrics(y_true, y_hat):\n",
    "    y_true = np.array(y_true)\n",
    "    y_hat = np.array(y_hat)\n",
    "\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "    rmsle = math.sqrt(((np.log1p(np.maximum(0, y_hat)) - np.log1p(np.maximum(0, y_true)))**2).mean())\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    mape = np.mean(np.abs((y_true - y_hat) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "    return rmse, rmsle, mae, mape, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec67fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Validation RMSE: 4458.3532, RMSLE: 0.0103, MAE: 3543.1254, MAPE: 0.82%, R²: 0.9990\n",
      "Step 2 Validation RMSE: 8425.9086, RMSLE: 0.0188, MAE: 6672.5978, MAPE: 1.52%, R²: 0.9964\n",
      "Step 3 Validation RMSE: 10510.6731, RMSLE: 0.0218, MAE: 8299.3399, MAPE: 1.82%, R²: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preds = lgbm_model.predict(\n",
    "    n=n_predict,\n",
    "    series=train_series,\n",
    "    past_covariates=train_pasts,\n",
    "    future_covariates=test_futures  # includes July covariates\n",
    ")\n",
    "# preds is a list/sequence of TimeSeries (one per input series)\n",
    "# invert transforms per state and compute metrics\n",
    "import pandas as pd\n",
    "y_true = []\n",
    "y_hat = []\n",
    "\n",
    "for j in range (n_predict):\n",
    "    for i, sname in enumerate(ts_transformed):\n",
    "        # preds[i] corresponds to series_list order; be careful with alignment\n",
    "        pred_ts = preds[i][j]\n",
    "        # invert transform using the state's pipeline\n",
    "        inv = pipeline_dict[sname].inverse_transform(pred_ts)\n",
    "        # extract scalar value\n",
    "        y_hat.append(inv.values()[-1].item())   # predicted next-month price\n",
    "        # true next-month value (from val_series)\n",
    "        true_val = val_series[i][j]\n",
    "        true_inv = pipeline_dict[sname].inverse_transform(true_val)\n",
    "        y_true.append(true_inv.values()[-1].item())\n",
    "\n",
    "    rmse, rmsle, mae, mape, r2 = calculate_metrics(y_true, y_hat)\n",
    "    print(f\"Step {j+1} Validation RMSE: {rmse:.4f}, RMSLE: {rmsle:.4f}, \"\n",
    "              f\"MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "    y_hat = []\n",
    "    y_true = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d32432",
   "metadata": {},
   "source": [
    "# Predictions on AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4a76f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = []\n",
    "train_pasts = []\n",
    "train_futures = []\n",
    "test_futures = []\n",
    "\n",
    "for s in ts_transformed:\n",
    "    ts = ts_transformed[s]\n",
    "    train = ts\n",
    "    train_series.append(train)\n",
    "    # same slicing for covariates if present\n",
    "    if past_cov_ts[s] is not None:\n",
    "        train_pasts.append(past_cov_ts[s])\n",
    "    else:\n",
    "        train_pasts.append(None)\n",
    "    if future_cov_ts[s] is not None:\n",
    "        train_futures.append(future_cov_ts[s])\n",
    "        test_futures.append(future_cov_ts[s])\n",
    "    else:\n",
    "        train_futures.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7f09ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250262\n",
      "[LightGBM] [Info] Number of data points in the train set: 3672, number of used features: 998\n",
      "[LightGBM] [Info] Start training from score 0.646784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMModel(lags=12, lags_past_covariates=[-24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1], lags_future_covariates=[1], output_chunk_length=1, output_chunk_shift=0, add_encoders=None, likelihood=None, quantiles=None, random_state=42, multi_models=True, use_static_covariates=True, categorical_past_covariates=None, categorical_future_covariates=None, categorical_static_covariates=None)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lags = 12\n",
    "lags_past_covariates = list(range(-24,0))   # previous 24 months of past covariates\n",
    "lags_future_covariates = list(range(1, n_predict+1))  # months ahead (just 1)\n",
    "\n",
    "lgbm_model = LightGBMModel(\n",
    "    lags=lags,\n",
    "    lags_past_covariates=lags_past_covariates,\n",
    "    lags_future_covariates=lags_future_covariates,\n",
    "    output_chunk_length=n_predict,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on the training series list (global model trained across all states)\n",
    "lgbm_model.fit(series=train_series, past_covariates=train_pasts, future_covariates=train_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f77e862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "85b5594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9cb47463",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = pd.read_csv(\"data\\RDC_Inventory_Core_Metrics_State.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ff03d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = test_real.sort_values(['state']).reset_index(drop=True)\n",
    "test_real = test_real['median_listing_price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ecfee4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preds = lgbm_model.predict(\n",
    "    n=n_predict,\n",
    "    series=train_series,\n",
    "    past_covariates=train_pasts,\n",
    "    future_covariates=test_futures  # includes July covariates\n",
    ")\n",
    "# preds is a list/sequence of TimeSeries (one per input series)\n",
    "# invert transforms per state and compute metrics\n",
    "import pandas as pd\n",
    "y_true = []\n",
    "y_hat = []\n",
    "\n",
    "for i, sname in enumerate(ts_transformed):\n",
    "    # preds[i] corresponds to series_list order; be careful with alignment\n",
    "    pred_ts = preds[i]\n",
    "    # invert transform using the state's pipeline\n",
    "    inv = pipeline_dict[sname].inverse_transform(pred_ts)\n",
    "    # extract scalar value\n",
    "    y_hat.append(inv.values()[-1].item())   # predicted next-month price\n",
    "    # true next-month value (from val_series)\n",
    "    true_val = val_series[i]\n",
    "    true_inv = pipeline_dict[sname].inverse_transform(true_val)\n",
    "    y_true.append(true_inv.values()[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9a7a2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 6524.4285, RMSLE: 0.0126, MAE: 4475.2012, MAPE: 0.97%, R²: 0.9977\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true = np.array(test_real)\n",
    "y_hat = np.array(y_hat)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "rmsle = math.sqrt(((np.log1p(np.maximum(0, y_hat)) - np.log1p(np.maximum(0, y_true)))**2).mean())\n",
    "mae = mean_absolute_error(y_true, y_hat)\n",
    "mape = np.mean(np.abs((y_true - y_hat) / y_true)) * 100\n",
    "r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.4f}, RMSLE: {rmsle:.4f}, \"\n",
    "      f\"MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
