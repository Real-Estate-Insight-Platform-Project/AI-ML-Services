{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95e60087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer, Scaler, InvertibleMapper\n",
    "from darts.models import LightGBMModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d412e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb9a919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"data/State_Processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17c139b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 0. Setup DataFrame ------------------------------------------------\n",
    "# df is your DataFrame with columns: 'year','month','state', 'median_listing_price', ...others...\n",
    "# Ensure month/day produce a proper date (use first of month)\n",
    "df = new_df.copy()\n",
    "df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2) + '-01')\n",
    "# sort\n",
    "df = df.sort_values(['state','date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "976afdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0dd9332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 1. Create per-state TimeSeries ----------------------------------\n",
    "# We'll build a dict: one grouped TimeSeries object per state (Darts supports grouping)\n",
    "grouped_ts = {}\n",
    "grouped_static = {}   # optional static covariates per state\n",
    "\n",
    "for state, g in df.groupby('state'):\n",
    "    # create a TimeSeries with monthly frequency\n",
    "    ts = TimeSeries.from_dataframe(\n",
    "    g,\n",
    "    time_col=\"date\",\n",
    "    value_cols=\"median_listing_price\",\n",
    "    freq=\"MS\",\n",
    "    )\n",
    "    grouped_ts[state] = ts\n",
    "    \n",
    "    # optionally create a small static vector for the state (if columns exist)\n",
    "    # use first row of the group for static features\n",
    "    # static_cols = ['Region', 'Division']  # example static columns — adapt to your df\n",
    "    # static_row = g[static_cols].iloc[0].to_dict()\n",
    "    # grouped_static[state] = static_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a119c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 2. Create past and/or future covariates --------------------------\n",
    "# Choose covariates you consider useful and that exist per (state, month)\n",
    "# Example: use active_listing_count and pending_ratio as past covariates (monthly history)\n",
    "past_cov_ts = {}\n",
    "future_cov_ts = {}\n",
    "past_cov_cols = ['median_listing_price_mm',\n",
    "    'median_listing_price_yy', 'active_listing_count',\n",
    "    'active_listing_count_mm', 'active_listing_count_yy',\n",
    "    'median_days_on_market', 'median_days_on_market_mm',\n",
    "    'median_days_on_market_yy', 'new_listing_count', 'new_listing_count_mm',\n",
    "    'new_listing_count_yy', 'price_increased_count',\n",
    "    'price_increased_count_mm', 'price_increased_count_yy',\n",
    "    'price_increased_share', 'price_increased_share_mm',\n",
    "    'price_increased_share_yy', 'price_reduced_count',\n",
    "    'price_reduced_count_mm', 'price_reduced_count_yy',\n",
    "    'price_reduced_share', 'price_reduced_share_mm',\n",
    "    'price_reduced_share_yy', 'pending_listing_count',\n",
    "    'pending_listing_count_mm', 'pending_listing_count_yy',\n",
    "    'median_listing_price_per_square_foot',\n",
    "    'median_listing_price_per_square_foot_mm',\n",
    "    'median_listing_price_per_square_foot_yy', 'median_square_feet',\n",
    "    'median_square_feet_mm', 'median_square_feet_yy',\n",
    "    'average_listing_price', 'average_listing_price_mm',\n",
    "    'average_listing_price_yy', 'total_listing_count',\n",
    "    'total_listing_count_mm', 'total_listing_count_yy', 'pending_ratio',\n",
    "    'pending_ratio_mm', 'pending_ratio_yy']\n",
    "future_cov_cols = ['month','year']  # calendar features known ahead\n",
    "\n",
    "for state, g in df.groupby('state'):\n",
    "    # Past covariates as a multivariate TimeSeries\n",
    "    if all(c in g.columns for c in past_cov_cols):\n",
    "        past_cov_ts[state] = TimeSeries.from_dataframe(g, time_col='date', value_cols=past_cov_cols, freq='MS')\n",
    "    else:\n",
    "        past_cov_ts[state] = None\n",
    "\n",
    "    if all(c in g.columns for c in future_cov_cols):\n",
    "        future_cov_ts[state] = TimeSeries.from_dataframe(g, time_col='date', value_cols=future_cov_cols, freq='MS')\n",
    "    else:\n",
    "        future_cov_ts[state] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5ce6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 3. Build and fit preprocessing pipeline (per-state or global) ----\n",
    "# We'll use a per-state pipeline (keeps scaling per-state) or reuse one global pipeline if preferred.\n",
    "pipeline_dict = {}\n",
    "ts_transformed = {}\n",
    "\n",
    "for state in grouped_ts:\n",
    "    log_transformer = InvertibleMapper(np.log1p, np.expm1)   # log1p for target, invertible\n",
    "    scaler = Scaler()\n",
    "    pipe = Pipeline([log_transformer, scaler])\n",
    "    # fit_transform expects a TimeSeries (or list); we pass the one series\n",
    "    transformed = pipe.fit_transform(grouped_ts[state])\n",
    "    pipeline_dict[state] = pipe\n",
    "    ts_transformed[state] = transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f041d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 4. Prepare for training: create dictionaries for Darts model -------\n",
    "# Darts LightGBMModel can be fit on a list (or sequence) of series; collect them\n",
    "series_list = [ts_transformed[s] for s in ts_transformed]\n",
    "past_cov_list = [past_cov_ts[s] for s in ts_transformed]\n",
    "future_cov_list = [future_cov_ts[s] for s in ts_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "394aa171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 5. Train/validation split ---------------------------------------\n",
    "# Example: hold out the last month for validation for each state\n",
    "# We'll do a simple split: train on all but last month, validate on last month\n",
    "n_predict = 1  # next month\n",
    "train_series = []\n",
    "val_series = []\n",
    "train_pasts = []\n",
    "val_pasts = []\n",
    "train_futures = []\n",
    "val_futures = []\n",
    "test_futures = []\n",
    "\n",
    "for s in ts_transformed:\n",
    "    ts = ts_transformed[s]\n",
    "    # if len(ts) < 24:\n",
    "    #     # skip too-short series (optional) or handle differently\n",
    "    #     continue\n",
    "    train = ts[:-n_predict]\n",
    "    val = ts[-n_predict:]  # last month\n",
    "    train_series.append(train)\n",
    "    val_series.append(val)\n",
    "    # same slicing for covariates if present\n",
    "    if past_cov_ts[s] is not None:\n",
    "        train_pasts.append(past_cov_ts[s][:-n_predict])\n",
    "        val_pasts.append(past_cov_ts[s][-n_predict:])\n",
    "    else:\n",
    "        train_pasts.append(None)\n",
    "        val_pasts.append(None)\n",
    "    if future_cov_ts[s] is not None:\n",
    "        train_futures.append(future_cov_ts[s][:-n_predict])\n",
    "        val_futures.append(future_cov_ts[s][-n_predict:])\n",
    "        test_futures.append(future_cov_ts[s])  # for final test prediction\n",
    "    else:\n",
    "        train_futures.append(None)\n",
    "        val_futures.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6bf6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126707\n",
      "[LightGBM] [Info] Number of data points in the train set: 4233, number of used features: 506\n",
      "[LightGBM] [Info] Start training from score 0.576940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMModel(lags=12, lags_past_covariates=[-12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1], lags_future_covariates=[1], output_chunk_length=1, output_chunk_shift=0, add_encoders=None, likelihood=None, quantiles=None, random_state=None, multi_models=True, use_static_covariates=True, categorical_past_covariates=None, categorical_future_covariates=None, categorical_static_covariates=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------- 6. Instantiate and fit LightGBMModel -----------------------------\n",
    "# Choose lags in months (e.g., last 12 months)\n",
    "lags = 12\n",
    "lags_past_covariates = list(range(-12,0))   # previous 12 months of past covariates\n",
    "lags_future_covariates = list(range(1, n_predict+1))  # months ahead (just 1)\n",
    "\n",
    "lgbm_model = LightGBMModel(\n",
    "    lags=lags,\n",
    "    lags_past_covariates=lags_past_covariates,\n",
    "    lags_future_covariates=lags_future_covariates,\n",
    "    output_chunk_length=n_predict\n",
    ")\n",
    "\n",
    "# Fit on the training series list (global model trained across all states)\n",
    "lgbm_model.fit(series=train_series, past_covariates=train_pasts, future_covariates=train_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0791131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec67fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------- 7. Validate (predict the held-out month) -------------------------\n",
    "preds = lgbm_model.predict(\n",
    "    n=n_predict,\n",
    "    series=train_series,\n",
    "    past_covariates=train_pasts,\n",
    "    future_covariates=test_futures  # includes July covariates\n",
    ")\n",
    "# preds is a list/sequence of TimeSeries (one per input series)\n",
    "# invert transforms per state and compute metrics\n",
    "import pandas as pd\n",
    "y_true = []\n",
    "y_hat = []\n",
    "\n",
    "for i, sname in enumerate(ts_transformed):\n",
    "    # preds[i] corresponds to series_list order; be careful with alignment\n",
    "    pred_ts = preds[i]\n",
    "    # invert transform using the state's pipeline\n",
    "    inv = pipeline_dict[sname].inverse_transform(pred_ts)\n",
    "    # extract scalar value\n",
    "    y_hat.append(inv.values()[-1].item())   # predicted next-month price\n",
    "    # true next-month value (from val_series)\n",
    "    true_val = val_series[i]\n",
    "    true_inv = pipeline_dict[sname].inverse_transform(true_val)\n",
    "    y_true.append(true_inv.values()[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "699786d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 5651.7092, RMSLE: 0.0113, MAE: 4338.6464, MAPE: 0.93%, R²: 0.9983\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_hat = np.array(y_hat)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "rmsle = math.sqrt(((np.log1p(np.maximum(0, y_hat)) - np.log1p(np.maximum(0, y_true)))**2).mean())\n",
    "mae = mean_absolute_error(y_true, y_hat)\n",
    "mape = np.mean(np.abs((y_true - y_hat) / y_true)) * 100\n",
    "r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.4f}, RMSLE: {rmsle:.4f}, \"\n",
    "      f\"MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d32432",
   "metadata": {},
   "source": [
    "# Predictions on AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a76f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 5. Train/validation split ---------------------------------------\n",
    "# Example: hold out the last month for validation for each state\n",
    "# We'll do a simple split: train on all but last month, validate on last month\n",
    "\n",
    "train_series = []\n",
    "train_pasts = []\n",
    "train_futures = []\n",
    "test_futures = []\n",
    "\n",
    "for s in ts_transformed:\n",
    "    ts = ts_transformed[s]\n",
    "    train = ts\n",
    "    train_series.append(train)\n",
    "    # same slicing for covariates if present\n",
    "    if past_cov_ts[s] is not None:\n",
    "        train_pasts.append(past_cov_ts[s])\n",
    "    else:\n",
    "        train_pasts.append(None)\n",
    "    if future_cov_ts[s] is not None:\n",
    "        train_futures.append(future_cov_ts[s])\n",
    "        test_futures.append(future_cov_ts[s])\n",
    "    else:\n",
    "        train_futures.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f09ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126707\n",
      "[LightGBM] [Info] Number of data points in the train set: 4284, number of used features: 506\n",
      "[LightGBM] [Info] Start training from score 0.581223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMModel(lags=12, lags_past_covariates=[-12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1], lags_future_covariates=[1], output_chunk_length=1, output_chunk_shift=0, add_encoders=None, likelihood=None, quantiles=None, random_state=None, multi_models=True, use_static_covariates=True, categorical_past_covariates=None, categorical_future_covariates=None, categorical_static_covariates=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------- 6. Instantiate and fit LightGBMModel -----------------------------\n",
    "# Choose lags in months (e.g., last 12 months)\n",
    "lags = 12\n",
    "lags_past_covariates = list(range(-12,0))   # previous 12 months of past covariates\n",
    "lags_future_covariates = list(range(1, n_predict+1))  # months ahead (just 1)\n",
    "\n",
    "lgbm_model = LightGBMModel(\n",
    "    lags=lags,\n",
    "    lags_past_covariates=lags_past_covariates,\n",
    "    lags_future_covariates=lags_future_covariates,\n",
    "    output_chunk_length=n_predict\n",
    ")\n",
    "\n",
    "# Fit on the training series list (global model trained across all states)\n",
    "lgbm_model.fit(series=train_series, past_covariates=train_pasts, future_covariates=train_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f77e862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "85b5594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "\n",
    "# Extend each state's future covariates separately\n",
    "for i, ts in enumerate(test_futures):\n",
    "    if ts is None:\n",
    "        continue\n",
    "    last_date = ts.end_time()\n",
    "    future_ext = pd.date_range(last_date + pd.offsets.MonthBegin(1), periods=n_predict, freq=\"MS\")\n",
    "    \n",
    "    extra_covs = pd.DataFrame({\n",
    "        \"year\": future_ext.year,\n",
    "        \"month\": future_ext.month,\n",
    "    }, index=future_ext)\n",
    "    \n",
    "    extra_covs_ts = TimeSeries.from_dataframe(extra_covs)\n",
    "    test_futures[i] = ts.append(extra_covs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cb47463",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = pd.read_csv(\"data\\RDC_Inventory_Core_Metrics_State.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff03d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = test_real.sort_values(['state']).reset_index(drop=True)\n",
    "test_real = test_real['median_listing_price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecfee4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1 Disk D\\Mora Academic\\Sem 5\\Data Science and Engineering Project\\AI-ML-Services\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------- 7. Validate (predict the held-out month) -------------------------\n",
    "preds = lgbm_model.predict(\n",
    "    n=n_predict,\n",
    "    series=train_series,\n",
    "    past_covariates=train_pasts,\n",
    "    future_covariates=test_futures  # includes July covariates\n",
    ")\n",
    "# preds is a list/sequence of TimeSeries (one per input series)\n",
    "# invert transforms per state and compute metrics\n",
    "import pandas as pd\n",
    "y_true = []\n",
    "y_hat = []\n",
    "\n",
    "for i, sname in enumerate(ts_transformed):\n",
    "    # preds[i] corresponds to series_list order; be careful with alignment\n",
    "    pred_ts = preds[i]\n",
    "    # invert transform using the state's pipeline\n",
    "    inv = pipeline_dict[sname].inverse_transform(pred_ts)\n",
    "    # extract scalar value\n",
    "    y_hat.append(inv.values()[-1].item())   # predicted next-month price\n",
    "    # true next-month value (from val_series)\n",
    "    true_val = val_series[i]\n",
    "    true_inv = pipeline_dict[sname].inverse_transform(true_val)\n",
    "    y_true.append(true_inv.values()[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a7a2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 6979.1414, RMSLE: 0.0143, MAE: 4721.6228, MAPE: 1.05%, R²: 0.9973\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true = np.array(test_real)\n",
    "y_hat = np.array(y_hat)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "rmsle = math.sqrt(((np.log1p(np.maximum(0, y_hat)) - np.log1p(np.maximum(0, y_true)))**2).mean())\n",
    "mae = mean_absolute_error(y_true, y_hat)\n",
    "mape = np.mean(np.abs((y_true - y_hat) / y_true)) * 100\n",
    "r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.4f}, RMSLE: {rmsle:.4f}, \"\n",
    "      f\"MAE: {mae:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1ef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea37ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668846d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f2311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
